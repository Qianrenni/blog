"use strict";(self["webpackChunkmyblog"]=self["webpackChunkmyblog"]||[]).push([[8521],{98521:function(n,e,t){t.r(e),e["default"]='# ç›®å½•\n\n- [ä¸€,å…¨å±€ç¯å¢ƒå‚æ•°å®šä¹‰](#ä¸€å…¨å±€ç¯å¢ƒå‚æ•°å®šä¹‰)\n- [äºŒ,å·¥å…·å‡½æ•°å’Œå¥–åŠ±å‡½æ•°å®šä¹‰](#äºŒå·¥å…·å‡½æ•°å’Œå¥–åŠ±å‡½æ•°å®šä¹‰)\n- [ä¸‰,è¿­ä»£æ–¹æ³•](#ä¸‰è¿­ä»£æ–¹æ³•)\n  - [1.è’™ç‰¹å¡æ´›æ–¹æ³•](#1è’™ç‰¹å¡æ´›æ–¹æ³•)\n    - [1.1 ç­–ç•¥ç”Ÿæˆ](#11-ç­–ç•¥ç”Ÿæˆ)\n    - [1.2 On-policy æ–¹æ³•](#12-on-policy-æ–¹æ³•)\n    - [1.3 Off-policy æ–¹æ³•](#13-off-policy-æ–¹æ³•)\n  - [2.åŠ¨æ€è§„åˆ’æ–¹æ³•](#2åŠ¨æ€è§„åˆ’æ–¹æ³•)\n    - [2.1  ç­–ç•¥è¯„ä¼°](#21--ç­–ç•¥è¯„ä¼°)\n    - [2.2 ä»·å€¼è¿­ä»£](#22-ä»·å€¼è¿­ä»£)\n  - [3.æ—¶åˆ†åºåˆ—æ–¹æ³•](#3æ—¶åˆ†åºåˆ—æ–¹æ³•)\n    - [3.1 Q-learning](#31-q-learning)\n    - [3.2 SARSA](#32-sarsa)\n    - [3.3 N-Sarsa](#33-n-sarsa)\n- [å››,è¾“å‡ºç­–ç•¥å’Œä»·å€¼](#å››è¾“å‡ºç­–ç•¥å’Œä»·å€¼)\n- [äº”,è°ƒè¯•](#äº”è°ƒè¯•)\n- [å…­,ç½‘ç»œæ¨¡å‹](#å…­ç½‘ç»œæ¨¡å‹)\n\n# ä¸€,å…¨å±€ç¯å¢ƒå‚æ•°å®šä¹‰\n\n\n```python\nimport random\n\nimport numpy as np\nfrom collections import defaultdict\n\n# å‚æ•°è®¾ç½®ï¼ˆå¤ç”¨ä¹‹å‰çš„å®šä¹‰ï¼‰\nSIZE = 9\nACTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]\nACTIONS_NAMES = [\'â†‘\', \'â†“\', \'â†\', \'â†’\']\nDOUBLE_ACTIONS={frozenset(\'â†‘â†“\'):\'â†•\',frozenset(\'â†‘â†\'):\'â†‘â†\',frozenset(\'â†‘â†’\'):\'â†‘â†’\',\n                frozenset(\'â†â†’\'):\'â†”\',frozenset(\'â†â†“\'):\'â†â†“\',frozenset(\'â†“â†’\'):\'â†“â†’\'}\nNUM_ACTIONS = len(ACTIONS)\nGOAL_STATE= {80,10,50}\nDANGER_STATES = {5,25,45,75}  # å±é™©åŒºåŸŸ\nCHEST_STATES = {15:1,35:2,55:4}  # é”®ä¸ºçŠ¶æ€ç¼–å·ï¼Œå€¼ä¸ºå¯¹åº”çš„äºŒè¿›åˆ¶ä½\nFULL_CHEST_MASK = sum(CHEST_STATES.values())\nGAMMA = 0.9\nNUM_EPISODES = 5000  # è®­ç»ƒè½®æ•°\nEPSILON = 0.1  # æ¢ç´¢ç‡\nEPSILON_START = 1.0\nEPSILON_END = 0.01\nEPSILON_DECAY = 0.995  # æ¯ä¸€è½®ä¹˜è¿™ä¸ªå› å­\nTHRESHOLD=1e-4\n```\n\n# äºŒ,å·¥å…·å‡½æ•°å’Œå¥–åŠ±å‡½æ•°å®šä¹‰\n\n\n```python\n\ndef to_state(row, col):\n    return row * SIZE + col\n\ndef from_state(s):\n    return np.divmod(s, SIZE)\ndef is_terminal(s, mask):\n    return s in GOAL_STATE and mask == FULL_CHEST_MASK\n\ndef step(s, mask, a):\n    # ä¸å‰é¢ä¸€è‡´ï¼šè¿”å› next_s, next_mask, reward, done\n    row, col = divmod(s, SIZE)\n    dr, dc = ACTIONS[a]\n    new_row = max(0, min(SIZE - 1, row + dr))\n    new_col = max(0, min(SIZE - 1, col + dc))\n    # if new_row==row and new_col==col:\n    #     return s, mask, -100, False\n    next_s = to_state(new_row, new_col)\n\n    next_mask = mask\n    reward =-1\n    done = False\n\n    if next_s in GOAL_STATE:\n        done = True\n        if mask == FULL_CHEST_MASK:\n            reward = 1000\n        else:\n            reward = -100\n            return s, mask, reward, done  # å›åŸåœ°\n\n    elif next_s in DANGER_STATES:\n        reward = -100\n\n    elif next_s in CHEST_STATES:\n        bit = CHEST_STATES[next_s]\n        if not (next_mask & bit):\n            next_mask = mask | bit\n            reward = 50\n        else:\n            reward = -1\n\n    return next_s, next_mask, reward, done\n\n```\n\n# ä¸‰,è¿­ä»£æ–¹æ³•\n\n## 1.è’™ç‰¹å¡æ´›æ–¹æ³•\n\n### 1.1 ç­–ç•¥ç”Ÿæˆ\n\n\n```python\n\ndef generate_episode(policy, epsilon=EPSILON):\n    # ç”Ÿæˆä¸€ä¸ª episode\n    episode = []\n    s = np.random.randint(SIZE * SIZE)\n    mask = 0\n    for _ in range(100):  # æœ€å¤§é•¿åº¦é™åˆ¶\n        if is_terminal(s, mask):\n            break\n        # Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œ\n        if np.random.rand() < epsilon:\n            a = np.random.choice(NUM_ACTIONS)\n        else:\n            a = policy[s, mask]\n        next_s, next_mask, r, done = step(s, mask, a)\n        episode.append((s, mask, a, r))\n        s, mask = next_s, next_mask\n    return episode\n\n# åŠ¨æ€ Îµ-greedy ç­–ç•¥ç”Ÿæˆ episode\n# ========================\ndef generate_episode_with_dynamic_epsilon(Q, epsilon):\n    episode = []\n    s = np.random.randint(SIZE * SIZE)\n    mask = 0\n    for _ in range(100):  # é˜²æ­¢æ­»å¾ªç¯\n        if is_terminal(s, mask):\n            break\n        # Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œ\n        if np.random.rand() < epsilon:\n            a = np.random.choice(NUM_ACTIONS)\n        else:\n            values = [Q.get((s, mask, ac), 0) for ac in range(NUM_ACTIONS)]\n            a = np.argmax(values) if max(values)!=0 else np.random.choice(NUM_ACTIONS)\n        next_s, next_mask, r, done = step(s, mask, a)\n        episode.append((s, mask, a, r))\n        s, mask = next_s, next_mask\n    return episode\n```\n\n### 1.2 On-policy æ–¹æ³•\n\n\n```python\n\ndef on_policy_monte_carlo_control():\n    num_states = SIZE * SIZE\n    num_masks = FULL_CHEST_MASK + 1\n\n    # åˆå§‹åŒ– Q è¡¨å’Œç­–ç•¥\n    Q = defaultdict(float)\n    C = defaultdict(float)\n    policy = np.zeros(shape=(num_states, num_masks), dtype=int)\n    episode_returns = []\n    for ep in range(NUM_EPISODES):\n        episode = generate_episode(policy,max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** ep)))\n\n        G = 0\n        visited_sa = set()\n\n        # ä»åå¾€å‰è®¡ç®— G\n        for t in reversed(range(len(episode))):\n            s, mask, a, r = episode[t]\n            G = GAMMA * G + r\n            sa = (s, mask, a)\n\n            if sa not in visited_sa:\n                visited_sa.add(sa)\n                C[sa] += 1\n                Q[sa] += (G - Q[sa]) / C[sa]  # å¢é‡å¹³å‡æ›´æ–°\n        episode_returns.append(G)\n        # ç­–ç•¥æ”¹è¿›ï¼ˆgreedyï¼‰\n        for s in range(num_states):\n            for mask in range(num_masks):\n                if is_terminal(s, mask):\n                    continue\n                values = [Q.get((s, mask, a), 0) for a in range(NUM_ACTIONS)]\n                best_a = np.argmax(values)\n                policy[s, mask] = best_a\n\n        if ep % 1000 == 0:\n            print(f"Episode {ep} completed")\n\n    return policy, Q , episode_returns\n```\n\n### 1.3 Off-policy æ–¹æ³•\n\n\n```python\n\n# ========================\n# Off-policy First-Visit MC Control\n# ========================\ndef off_policy_monte_carlo():\n    num_states = SIZE * SIZE\n    num_masks = FULL_CHEST_MASK + 1\n\n    # åˆå§‹åŒ–\n    Q = defaultdict(float)\n    C = defaultdict(float)\n    target_policy = np.random.choice(NUM_ACTIONS, size=(num_states, num_masks))\n    episode_returns = []\n    for ep in range(NUM_EPISODES):\n        # åŠ¨æ€è°ƒæ•´ epsilon\n        epsilon = max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** ep))\n        # ä½¿ç”¨å›ºå®šç­–ç•¥ Ï€_bï¼ˆbehavior policyï¼‰æ”¶é›†æ•°æ®\n        episode = generate_episode_with_dynamic_epsilon(Q, epsilon)\n        G = 0.0\n        W = 1.0  # é‡è¦æ€§é‡‡æ ·æ¯”ç‡\n        visited_sa = set()\n\n        # ä»åå¾€å‰å¤„ç†æ¯ä¸ªæ—¶é—´æ­¥\n        for t in reversed(range(len(episode))):\n            s, mask, a, r = episode[t]\n            G = GAMMA * G + r\n            sa = (s, mask, a)\n\n            if sa not in visited_sa:\n                visited_sa.add(sa)\n\n                # æ›´æ–°é‡è¦æ€§é‡‡æ ·ç»Ÿè®¡é‡\n                C[sa] += W\n\n                # å¢é‡å¹³å‡æ›´æ–° Q å€¼\n                Q[sa] += (W / C[sa]) * (G - Q[sa])\n\n                # æ›´æ–°ç›®æ ‡ç­–ç•¥ä¸º greedy\n                values = [Q.get((s, mask, ac), 0) for ac in range(NUM_ACTIONS)]\n                best_a = np.argmax(values)\n                target_policy[s, mask] = best_a\n\n                # å¦‚æœå½“å‰åŠ¨ä½œä¸æ˜¯ç›®æ ‡ç­–ç•¥é€‰çš„åŠ¨ä½œï¼Œåˆ™ç»ˆæ­¢æ›´æ–°è·¯å¾„\n                if a != target_policy[s, mask]:\n                    W = 1e-8  # æƒé‡å½’é›¶ï¼Œä¸å†æ›´æ–°å‰é¢çš„çŠ¶æ€\n                    break\n            else:\n                continue\n        episode_returns.append(G)\n        if ep % 500 == 0:\n            print(f"Episode {ep} completed")\n\n    return target_policy, Q,  episode_returns\n```\n\n## 2.åŠ¨æ€è§„åˆ’æ–¹æ³•\n\n### 2.1  ç­–ç•¥è¯„ä¼°\n\n\n```python\n\ndef policy_evaluation(policy,V,env_step=step):\n    value_deltas = []\n    while True:\n        delta = 0\n        V_new=np.zeros_like(V)\n        for s in range(SIZE*SIZE):\n            for mask in range(FULL_CHEST_MASK + 1):\n                if is_terminal(s, mask):  # å¦‚æœæ˜¯ç»ˆç‚¹ä¸”å®ç®±é›†é½\n                    continue\n                v = V[s, mask]\n                a = policy[s, mask]\n\n                # ç¯å¢ƒè½¬ç§»å‡½æ•°åº”æ”¹ä¸ºæ¥å— (s, mask, a) è¿”å› (ns, nmask, r, done)\n                ns, nmask, r, done = env_step(s, mask, a)\n                V_new[s,mask]=r + GAMMA * V[ns, nmask]\n                delta = max(delta, abs(v - V_new[s, mask]))\n        value_deltas.append(delta)\n        V = V_new\n        if delta < THRESHOLD:\n            break\n\n    return V, value_deltas\ndef policy_improvement(policy,V,env_step=step):\n    stable = True\n    for s in range(SIZE*SIZE):\n        for mask in range(FULL_CHEST_MASK+1):\n            if is_terminal(s, mask):\n                continue\n            old_action = policy[s, mask]\n            values = []\n            for a in range(NUM_ACTIONS):\n                ns, nmask, r, done = env_step(s, mask, a)\n                values.append(r + GAMMA * V[ns, nmask])\n            best_a = np.argmax(values)\n            # å…³é”®ç‚¹ ä¸¤ä¸ªå€¼ç›¸å·®ä¸å¤§ä¼šå½±å“ ç­–ç•¥è¯„ä¼°,ä»è€Œå¯¼è‡´éœ‡è¡\n            if best_a != old_action and values[best_a] - values[old_action] >0.2:\n                policy[s, mask] = best_a\n                stable = False\n    return policy, stable\ndef policy_iteration():\n    # åˆå§‹åŒ–ç­–ç•¥å’Œä»·å€¼å‡½æ•°ï¼ˆç°åœ¨æ˜¯äºŒç»´ï¼‰\n    # æ‰©å±•çŠ¶æ€æ€»æ•° = åŸå§‹çŠ¶æ€æ•° Ã— å®ç®±æ©ç æ•°é‡\n    num_states = SIZE * SIZE\n    num_masks = FULL_CHEST_MASK + 1  # ä» 0 åˆ° FULL_CHEST_MASK\n    policy = np.zeros(shape=(num_states, num_masks), dtype=int)\n    V = np.zeros(shape=(num_states, num_masks))\n    deltas_history = []\n    for i in range(100):\n        V,deltas=policy_evaluation(policy,V)\n        deltas_history.extend(deltas)\n        policy,stable=policy_improvement(policy,V)\n        if stable:\n            print(f\'Policy iteration converged at iteration {i}\')\n            break\n    return policy,V,deltas_history\n```\n\n### 2.2 ä»·å€¼è¿­ä»£\n\n\n```python\n\ndef value_iteration():\n    num_state=SIZE*SIZE\n    V=np.zeros(shape=(num_state,FULL_CHEST_MASK+1))\n    policy=np.zeros(shape=(num_state,FULL_CHEST_MASK+1),dtype=int)\n    deltas_history=[]\n    while True:\n        delta=0\n        V_new=np.zeros_like(V)\n        for s in range(num_state):\n            for mask  in range(FULL_CHEST_MASK+1):\n                if is_terminal(s,mask):\n                    continue\n                values=[]\n                for a in range(NUM_ACTIONS):\n                    next_state,next_mask,reward,done=step(s,mask,a)\n                    if not done:\n                        values.append(reward+GAMMA*V[next_state,next_mask])\n                    else:\n                        values.append(reward)\n                new_value=max(values)\n                delta=max(delta,abs(new_value-V[s,mask]))\n                V_new[s,mask]=new_value\n        deltas_history.append(delta)\n        V=V_new\n        if delta<THRESHOLD:\n            print(f\'Value iteration converged at iteration {len(deltas_history)}\')\n            break\n\n    for s in range(num_state):\n        for mask in range(FULL_CHEST_MASK+1):\n            if is_terminal(s,mask):\n                policy[s,mask]=-1\n                continue\n            values=[]\n            for a in range(NUM_ACTIONS):\n                next_state,next_mask,reward,done=step(s,mask,a)\n                values.append(reward+GAMMA*V[next_state,next_mask])\n            policy[s,mask]=np.argmax(values)\n    return policy,V,deltas_history\n```\n\n## 3.æ—¶åˆ†åºåˆ—æ–¹æ³•\n\n###  3.1 Q-learning\n\n\n```python\ndef q_learing():\n    import numpy as np\n    # å‡è®¾ä½ æœ‰ SIZE*SIZE ä¸ªçŠ¶æ€ï¼ŒFULL_CHEST_MASK+1 ç§ maskï¼ŒNUM_ACTIONS ä¸ªåŠ¨ä½œ\n    Q = np.zeros(shape=(SIZE*SIZE, FULL_CHEST_MASK+1, NUM_ACTIONS))\n    GAMMA = 0.95     # æŠ˜æ‰£å› å­\n    ALPHA = 0.1      # å­¦ä¹ ç‡\n    EPSILON_START = 0.9\n    EPSILON_END = 0.1\n    EPSILON_DECAY = 0.995\n    EPISODES = 5000 # æ€»å…±è®­ç»ƒå¤šå°‘ episode\n    MAX_STEPS = 100  # æ¯ä¸ª episode æœ€å¤§æ­¥æ•°\n    def choose_action(state, mask, Q, epsilon):\n        if np.random.rand() < epsilon:\n            return np.random.randint(NUM_ACTIONS)\n        else:\n            return np.argmax(Q[state, mask])\n    def enviroment_reset():\n        return np.random.randint(SIZE*SIZE)\n    def initial_mask(initial_state):\n        return 0 if initial_state not in CHEST_STATES else CHEST_STATES[initial_state]\n    gradient_history = []\n    for episode in range(EPISODES):\n        state = enviroment_reset()\n        mask = initial_mask(state)\n        delta=0\n        for _ in range(MAX_STEPS):\n            if state in GOAL_STATE:\n                break\n            epsilon = max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** episode))\n            action = choose_action(state, mask, Q, epsilon)\n\n            # ä¸ç¯å¢ƒäº¤äº’å¾—åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€ç­‰ä¿¡æ¯\n            next_state, next_mask, reward, done = step(state,mask,action)\n\n            # Q-learning æ›´æ–°å…¬å¼\n            td_target = (reward + GAMMA * np.max(Q[next_state, next_mask]) )if not done else reward\n            td_error = td_target - Q[state, mask, action]\n            Q[state, mask, action] += ALPHA * td_error\n            delta=max(delta,abs(td_error))\n            state = next_state\n            mask = next_mask\n            if done:\n                break\n        gradient_history.append(delta)\n    return np.argmax(Q,  axis=2),  Q, gradient_history\n```\n\n### 3.2 SARSA\n\n\n```python\n\ndef sarsa():\n    import numpy as np\n    from collections import defaultdict\n\n    # è¶…å‚æ•°\n    NUM_EPISODES = 5000\n    GAMMA = 0.99\n    ALPHA = 0.1      # å­¦ä¹ ç‡\n    EPSILON_START = 0.5\n    EPSILON_END = 0.01\n    EPSILON_DECAY = 0.995\n    num_states = SIZE * SIZE\n    num_masks = FULL_CHEST_MASK + 1\n    def enviroment_reset():\n        state=np.random.randint(SIZE*SIZE)\n        return state,0 if state not in CHEST_STATES else CHEST_STATES[state]\n    # åˆå§‹åŒ– Q è¡¨æ ¼\n    Q = np.zeros(shape=(num_states, num_masks, NUM_ACTIONS))\n    episode_returns = []\n\n    for ep in range(NUM_EPISODES):\n        epsilon = max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** ep))\n\n        # é‡ç½®ç¯å¢ƒï¼ˆä½ éœ€è¦è‡ªå·±å®ç° reset å‡½æ•°ï¼‰\n        state, mask = enviroment_reset()  # è¿”å›åˆå§‹ state å’Œ mask\n\n        # Îµ-greedy é€‰æ‹©ç¬¬ä¸€ä¸ªåŠ¨ä½œ\n        if np.random.rand() < epsilon:\n            action = np.random.choice(NUM_ACTIONS)\n        else:\n            action = np.argmax(Q[(state, mask)])\n\n        done = state in GOAL_STATE\n        total_return = 0\n\n        while not done:\n            # æ‰§è¡Œå½“å‰åŠ¨ä½œï¼Œå¾—åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€ã€å¥–åŠ±ç­‰\n            next_state, next_mask, reward, done = step(state, mask, action)\n\n            # Îµ-greedy é€‰æ‹©ä¸‹ä¸€ä¸ªåŠ¨ä½œ\n            if np.random.rand() < epsilon:\n                next_action = np.random.choice(NUM_ACTIONS)\n            else:\n                next_action = np.argmax(Q[(next_state, next_mask)])\n\n            # SARSA æ›´æ–°å…¬å¼\n            td_target = reward + GAMMA * Q[(next_state, next_mask)][next_action]\n            td_error = td_target - Q[(state, mask)][action]\n            Q[(state, mask)][action] += ALPHA * td_error\n\n            # ç´¯è®¡å›æŠ¥\n            total_return += td_error\n\n            # æ›´æ–°çŠ¶æ€å’ŒåŠ¨ä½œ\n            state, mask = next_state, next_mask\n            action = next_action\n\n        episode_returns.append(total_return)\n\n        if ep % 500 == 0:\n            print(f"Episode {ep} completed")\n\n    return np.argmax(Q,  axis=2),Q, episode_returns\n```\n\n### 3.3 N-Sarsa\n\n\n```python\ndef n_step_sarsa(n=3, NUM_EPISODES=5000):\n    import numpy as np\n    from collections import defaultdict\n\n    # è¶…å‚æ•°\n    GAMMA = 0.99\n    ALPHA = 0.1      # å­¦ä¹ ç‡\n    EPSILON_START = 0.5\n    EPSILON_END = 0.01\n    EPSILON_DECAY = 0.995\n    num_states = SIZE * SIZE\n    num_masks = FULL_CHEST_MASK + 1\n    MAX_EPISODE_LENGTH = num_states+1  # è®¾ç½®æœ€å¤§æ­¥æ•°\n    def enviroment_reset():\n        state=np.random.randint(SIZE*SIZE)\n        return state,0 if state not in CHEST_STATES else CHEST_STATES[state]\n    # åˆå§‹åŒ– Q è¡¨æ ¼ï¼šQ[state][mask][action]\n    Q = np.zeros(shape=(num_states, num_masks, NUM_ACTIONS))\n\n    episode_returns = []\n\n    for ep in range(NUM_EPISODES):\n        epsilon = max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** ep))\n\n        # åˆå§‹åŒ–ç¯å¢ƒ\n        state, mask = enviroment_reset()  # è¿”å›åˆå§‹ state å’Œ mask\n        done = state in GOAL_STATE\n\n        # Îµ-greedy é€‰æ‹©ç¬¬ä¸€ä¸ªåŠ¨ä½œ\n        if np.random.rand() < epsilon:\n            action = np.random.choice(NUM_ACTIONS)\n        else:\n            action = np.argmax(Q[state, mask])\n\n        # å­˜å‚¨è½¨è¿¹ä¿¡æ¯\n        states = [state]\n        masks = [mask]\n        actions = [action]\n        rewards = []\n\n        T = float(\'inf\')  # episode çš„æ€»æ­¥æ•°ï¼ˆè¿˜æœªçŸ¥ï¼‰\n        t = 0  # å½“å‰æ—¶é—´æ­¥\n        # ç´¯è®¡æœ¬ episode çš„å›æŠ¥\n        total_return = 0\n        while True:\n            if t < T:\n                # æ‰§è¡ŒåŠ¨ä½œï¼Œå¾—åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€ã€å¥–åŠ±ç­‰\n                next_state, next_mask, reward, done = step(state, mask, action)\n                next_action=None\n                # å­˜å‚¨å¥–åŠ±\n                rewards.append(reward)\n\n                # å¦‚æœæœªç»“æŸï¼Œç»§ç»­é€‰åŠ¨ä½œ\n                if not done or t < MAX_EPISODE_LENGTH:\n                    if np.random.rand() < epsilon:\n                        next_action = np.random.choice(NUM_ACTIONS)\n                    else:\n                        next_action = np.argmax(Q[next_state, next_mask])\n                    # å­˜å‚¨ä¸‹ä¸€ä¸ªçŠ¶æ€å’ŒåŠ¨ä½œ\n                    states.append(next_state)\n                    masks.append(next_mask)\n                    actions.append(next_action)\n                else:\n                    T = t + 1  # è®¾ç½® episode ç»“æŸæ—¶é—´\n\n                # æ›´æ–°çŠ¶æ€å’ŒåŠ¨ä½œ\n                state, mask, action = next_state, next_mask, next_action\n\n            # æ›´æ–°ç›®æ ‡æ—¶é—´ç‚¹ Ï„\n            tau = t - n + 1\n            if tau >= 0:\n                # è®¡ç®— G_t^(n)ï¼šn-step return\n                end = min(tau + n, T)\n                G = sum([GAMMA ** (k - tau) * rewards[k] for k in range(tau, end)])\n\n                if end < T:\n                    G += GAMMA ** n * Q[states[end], masks[end]][actions[end]]\n                delta=ALPHA * (\n                    G - Q[states[tau], masks[tau], actions[tau]]\n                )\n                # æ›´æ–° Q å€¼\n                Q[states[tau], masks[tau], actions[tau]] += delta\n                total_return += delta/len(states)\n\n            if tau == T - 1:\n                break\n\n            t += 1\n\n        episode_returns.append(total_return)\n\n        if ep % 500 == 0:\n            print(f"Episode {ep} completed")\n\n    # è¿”å›æœ€ä¼˜ç­–ç•¥ï¼ˆå– argmaxï¼‰å’Œ Q è¡¨\n    return np.argmax(Q, axis=2), Q, episode_returns\n```\n\n# å››,è¾“å‡ºç­–ç•¥å’Œä»·å€¼\n\n\n```python\n\ndef print_policy(policy,mask=0):\n    for i in range(SIZE):\n        row = ""\n        for j in range(SIZE):\n            s = to_state(i, j)\n            if s in GOAL_STATE:\n                cell = "G"\n            elif s in DANGER_STATES:\n                cell = "X"\n            elif s in CHEST_STATES:\n                cell = CHEST_STATES[s]\n            else:\n                cell = ACTIONS_NAMES[policy[s, mask]]\n            row += f"{cell:^5}"\n        print(row)\ndef print_values(V,mask=0):\n    for i in range(SIZE):\n        row = ""\n        for j in range(SIZE):\n            s = to_state(i, j)\n            # if s in GOAL_STATE:\n            #     cell = \'G\'\n            # elif s in DANGER_STATES:\n            #     cell = \'X\'\n            # elif s in CHEST_STATES:\n            #     cell = f"$"\n            # else:\n            cell = f"{V[s,mask]:.2f}"\n            row += f"{cell:^7}"  # å±…ä¸­å¯¹é½ï¼Œæ¯ä¸ªæ•°å­—å 7ä¸ªå­—ç¬¦å®½åº¦\n        print(row)\ndef print_environment():\n    for i in range(SIZE):\n        row = ""\n        for j in range(SIZE):\n            s = to_state(i, j)\n            cell = "0"\n            if s in GOAL_STATE:\n                cell = \'G\'\n            elif s in DANGER_STATES:\n                cell = \'X\'\n            elif s in CHEST_STATES:\n                cell = f"$"\n            else:\n                pass\n            row += f"{cell:^7}"  # å±…ä¸­å¯¹é½ï¼Œæ¯ä¸ªæ•°å­—å 7ä¸ªå­—ç¬¦å®½åº¦\n        print(row)\ndef print_path(x,y,policy):\n    path=[ ["0"  for _ in range(SIZE)] for _  in range(SIZE)]\n    state=to_state(x,y)\n    mask=0\n    if state in CHEST_STATES:\n        mask=CHEST_STATES[state]\n    times=0\n    while state not in GOAL_STATE:\n        times+=1\n        action=policy[state,mask]\n        next_state,mask,_,_=step(state,mask,action)\n        i,j=from_state(state)\n        actions=frozenset(ACTIONS_NAMES[action]+path[i][j])\n        path[i][j]=DOUBLE_ACTIONS.get(actions,ACTIONS_NAMES[action])\n        if state>=SIZE*SIZE or state<0 or state==next_state or times>SIZE*SIZE:\n            break\n        state=next_state\n    i,j=from_state(state)\n    path[i][j]="ğŸ"\n    path[x][y]="ğŸ"\n    for i in range(SIZE):\n        for j in range(SIZE):\n            path[i][j]=path[i][j].center(7)\n    print(f"\\n Start from {x,y} to Goal")\n    for row in path:\n        print("".join(row))\ndef moving_average(a, window_size=50):\n    return np.convolve(a, np.ones(window_size)/window_size, mode=\'valid\')\ndef show_gradient_history(gradient_history):\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(12,8))\n    plt.plot(moving_average(gradient_history))\n    plt.xlabel(\'Episode\')\n    plt.ylabel(\'TD Error\')\n    plt.title(\'TD Error History\')\n    plt.show()\n\n```\n\n# äº”,è°ƒè¯•\n\n\n```python\nprint("Environment:")\nprint_environment()\n\non_policy,on_Q,on_episode_returns=on_policy_monte_carlo_control()\nprint("\\n On Policy:")\nprint_policy(on_policy)\n\noff_policy, off_Q,off_episode_returns = off_policy_monte_carlo()\nprint("\\n Off Policy:")\nprint_policy(off_policy)\n\npolicy,V,deltas_history=policy_iteration()\nprint("\\n Policy Iteration:")\nprint_policy(policy)\n\nvalue_policy,value_V,value_deltas_history=value_iteration()\nprint("\\n Value Iteration:")\nprint_policy(value_policy)\nq_policy,Q,gradient_history=q_learing()\nprint("\\n Q-Learning:")\nprint_policy(q_policy)\nprint("\\n SARSA")\nsarsa_policy,sarsa_Q,sarsa_gradient_history=sarsa()\nprint_policy(sarsa_policy)\nprint("\\n N-Step SARSA")\nn_sarsa_policy,n_sarsa_Q,n_sarsa_gradient_history=n_step_sarsa()\nprint_policy(n_sarsa_policy)\n```\n\n    Environment:\n       0      0      0      0      0      X      0      0      0   \n       0      G      0      0      0      0      $      0      0   \n       0      0      0      0      0      0      0      X      0   \n       0      0      0      0      0      0      0      0      $   \n       0      0      0      0      0      0      0      0      0   \n       X      0      0      0      0      G      0      0      0   \n       0      $      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      X      0      0      0      0      G   \n    Episode 0 completed\n    Episode 1000 completed\n    Episode 2000 completed\n    Episode 3000 completed\n    Episode 4000 completed\n    \n     On Policy:\n      â†’    â†’    â†’    â†“    â†    X    â†’    â†’    â†“  \n      â†‘    G    â†“    â†’    â†’    â†’    1    â†’    â†“  \n      â†’    â†“    â†“    â†“    â†‘    â†‘    â†‘    X    â†“  \n      â†’    â†’    â†’    â†’    â†‘    â†‘    â†’    â†’    2  \n      â†‘    â†’    â†’    â†“    â†    â†‘    â†‘    â†    â†‘  \n      X    â†’    â†“    â†    â†    G    â†’    â†‘    â†  \n      â†’    4    â†    â†’    â†    â†    â†’    â†‘    â†‘  \n      â†’    â†‘    â†    â†    â†    â†    â†‘    â†‘    â†‘  \n      â†‘    â†’    â†‘    X    â†‘    â†“    â†‘    â†    G  \n    Episode 0 completed\n    Episode 500 completed\n    Episode 1000 completed\n    Episode 1500 completed\n    Episode 2000 completed\n    Episode 2500 completed\n    Episode 3000 completed\n    Episode 3500 completed\n    Episode 4000 completed\n    Episode 4500 completed\n    \n     Off Policy:\n      â†’    â†’    â†’    â†’    â†’    X    â†“    â†    â†  \n      â†‘    G    â†“    â†    â†’    â†’    1    â†‘    â†“  \n      â†‘    â†’    â†“    â†“    â†‘    â†    â†‘    X    â†“  \n      â†“    â†“    â†    â†’    â†‘    â†’    â†“    â†’    2  \n      â†’    â†’    â†’    â†‘    â†‘    â†’    â†’    â†’    â†‘  \n      X    â†‘    â†‘    â†‘    â†    G    â†‘    â†‘    â†‘  \n      â†’    4    â†’    â†“    â†’    â†’    â†‘    â†’    â†‘  \n      â†’    â†’    â†’    â†’    â†’    â†‘    â†‘    â†’    â†‘  \n      â†’    â†‘    â†‘    X    â†’    â†‘    â†    â†‘    G  \n    Policy iteration converged at iteration 13\n    \n     Policy Iteration:\n      â†’    â†’    â†’    â†’    â†“    X    â†“    â†“    â†  \n      â†“    G    â†’    â†’    â†’    â†’    1    â†    â†  \n      â†“    â†“    â†‘    â†‘    â†‘    â†‘    â†‘    X    â†“  \n      â†“    â†“    â†    â†‘    â†‘    â†‘    â†‘    â†’    2  \n      â†’    â†“    â†“    â†“    â†‘    â†‘    â†‘    â†’    â†‘  \n      X    â†“    â†“    â†“    â†“    G    â†‘    â†‘    â†‘  \n      â†’    4    â†    â†    â†    â†    â†‘    â†‘    â†‘  \n      â†‘    â†‘    â†‘    â†    â†    â†    â†‘    â†‘    â†‘  \n      â†’    â†‘    â†‘    X    â†‘    â†‘    â†’    â†‘    G  \n    Value iteration converged at iteration 27\n    \n     Value Iteration:\n      â†’    â†’    â†“    â†“    â†“    X    â†“    â†“    â†“  \n      â†“    G    â†’    â†’    â†’    â†’    1    â†    â†“  \n      â†“    â†“    â†‘    â†‘    â†‘    â†‘    â†‘    X    â†“  \n      â†“    â†“    â†“    â†‘    â†‘    â†‘    â†‘    â†’    2  \n      â†’    â†“    â†“    â†“    â†‘    â†‘    â†‘    â†‘    â†‘  \n      X    â†“    â†“    â†“    â†“    G    â†‘    â†‘    â†‘  \n      â†’    4    â†    â†    â†    â†    â†‘    â†‘    â†‘  \n      â†‘    â†‘    â†‘    â†‘    â†‘    â†‘    â†‘    â†‘    â†‘  \n      â†‘    â†‘    â†‘    X    â†‘    â†‘    â†‘    â†‘    G  \n    \n     Q-Learning:\n      â†’    â†’    â†“    â†’    â†“    X    â†“    â†“    â†  \n      â†‘    G    â†’    â†’    â†’    â†’    1    â†    â†  \n      â†“    â†“    â†‘    â†’    â†‘    â†‘    â†‘    X    â†“  \n      â†“    â†’    â†“    â†    â†‘    â†‘    â†‘    â†’    2  \n      â†’    â†“    â†“    â†“    â†    â†‘    â†’    â†’    â†‘  \n      X    â†“    â†“    â†    â†    G    â†‘    â†‘    â†‘  \n      â†’    4    â†    â†    â†    â†    â†    â†‘    â†‘  \n      â†‘    â†‘    â†‘    â†    â†‘    â†    â†    â†‘    â†‘  \n      â†’    â†‘    â†‘    X    â†‘    â†‘    â†    â†‘    G  \n    \n     SARSA\n    Episode 0 completed\n    Episode 500 completed\n    Episode 1000 completed\n    Episode 1500 completed\n    Episode 2000 completed\n    Episode 2500 completed\n    Episode 3000 completed\n    Episode 3500 completed\n    Episode 4000 completed\n    Episode 4500 completed\n      â†“    â†’    â†“    â†’    â†“    X    â†“    â†“    â†  \n      â†“    G    â†’    â†’    â†’    â†’    1    â†    â†  \n      â†“    â†“    â†‘    â†‘    â†‘    â†’    â†‘    X    â†“  \n      â†’    â†“    â†    â†    â†’    â†’    â†‘    â†’    2  \n      â†’    â†“    â†“    â†“    â†    â†‘    â†‘    â†’    â†‘  \n      X    â†“    â†    â†“    â†    G    â†’    â†’    â†‘  \n      â†’    4    â†    â†    â†    â†    â†’    â†’    â†‘  \n      â†’    â†‘    â†‘    â†    â†‘    â†    â†    â†’    â†‘  \n      â†’    â†‘    â†‘    X    â†‘    â†    â†‘    â†‘    G  \n    \n     N-Step SARSA\n    Episode 0 completed\n    Episode 500 completed\n    Episode 1000 completed\n    Episode 1500 completed\n    Episode 2000 completed\n    Episode 2500 completed\n    Episode 3000 completed\n    Episode 3500 completed\n    Episode 4000 completed\n    Episode 4500 completed\n      â†“    â†    â†    â†    â†“    X    â†“    â†    â†  \n      â†“    G    â†“    â†“    â†’    â†“    1    â†‘    â†  \n      â†“    â†’    â†’    â†“    â†“    â†“    â†‘    X    â†“  \n      â†’    â†’    â†“    â†’    â†’    â†“    â†    â†’    2  \n      â†‘    â†    â†“    â†“    â†’    â†’    â†“    â†’    â†‘  \n      X    â†“    â†    â†    â†    G    â†’    â†’    â†‘  \n      â†’    4    â†‘    â†    â†    â†    â†’    â†’    â†‘  \n      â†‘    â†‘    â†    â†’    â†‘    â†    â†‘    â†’    â†‘  \n      â†’    â†‘    â†    X    â†‘    â†’    â†‘    â†    G  \n    \n\n\n```python\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,8))\nplt.plot(moving_average(on_episode_returns), label=\'On-policy\')\nplt.plot(moving_average(off_episode_returns), label=\'Off-policy\')\nplt.xlabel(\'Episode\')\nplt.ylabel(\'Average Return\')\nplt.legend()\n\nplt.show()\n```\n\n\n    \n![png](output_28_0.png)\n    \n\n\n\n```python\nplt.figure(figsize=(12,8))\nplt.plot(value_deltas_history, label=\'value-policy\')\nplt.plot(deltas_history, label=\'policy\')\nplt.xlabel(\'Episode\')\nplt.ylabel(\'Delta Decay\')\nplt.legend()\nplt.show()\n```\n\n\n    \n![png](output_29_0.png)\n    \n\n\n\n```python\nplt.figure(figsize=(12,8))\nplt.plot(moving_average(gradient_history), label=\'Q-Learning\')\nplt.plot(moving_average(sarsa_gradient_history), label=\'SARSA\')\nplt.plot(moving_average(n_sarsa_gradient_history), label=\'N-Step SARSA\')\nplt.xlabel(\'Episode\')\nplt.ylabel(\'TD Error\')\nplt.legend()\nplt.show()\n```\n\n\n    \n![png](output_30_0.png)\n    \n\n\n\n```python\nstart_pos=(0,0)\nprint("\\n On Policy")\nprint_path(*start_pos,on_policy)\nprint("\\n Off Policy")\nprint_path(*start_pos,off_policy)\nprint("\\n Policy Iteration")\nprint_path(*start_pos,policy)\nprint("\\n Value Iteration")\nprint_path(*start_pos,value_policy)\nprint("\\n Q-Learning")\nprint_path(*start_pos,q_policy)\nprint("\\n SARSA")\nprint_path(*start_pos,sarsa_policy)\nprint("\\n N-Step SARSA")\nprint_path(*start_pos,n_sarsa_policy)\n```\n\n    \n     On Policy\n    \n     Start from (0, 0) to Goal\n       ğŸ      â†’      â†’      â†“      0      0      0      0      0   \n       0      0      0      â†’      â†’      â†’      â†’      â†’      â†“   \n       0      0      0      0      0      0      0      0      â†“   \n       0      0      0      0      0      0      0      0      â†“   \n       0      0      0      0      0      0      0      â†“      â†   \n       0      0      0      0      0      ğŸ      0      â†“      0   \n       0      â†’      â†’      â†’      â†’      â†‘      â†“      â†      0   \n       0      â†‘      â†      â†      â†      â†      â†      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n     Off Policy\n    \n     Start from (0, 0) to Goal\n       ğŸ      â†’      â†’      â†’      â†’      â†’      â†“      0      0   \n       0      0      0      0      0      0      â†’      â†’      â†“   \n       0      0      0      0      0      0      0      0      â†“   \n       0      0      0      0      0      0      0      0      â†“   \n       0      0      0      0      0      0      0      0      â†“   \n       0      0      â†’      â†’      â†’      ğŸ      0      â†“      â†   \n       0      â†’      â†‘â†     â†      â†      â†      â†      â†      0   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n     Policy Iteration\n    \n     Start from (0, 0) to Goal\n       ğŸ      â†’      â†’      â†’      â†“      0      0      0      0   \n       0      ğŸ      0      0      â†’      â†’      â†“      0      0   \n       0      â†‘      0      0      0      0      â†“      0      0   \n       0      â†‘      0      0      0      0      â†’      â†’      â†“   \n       0      â†‘      0      0      0      0      0      0      â†“   \n       0      â†‘      0      0      0      0      0      0      â†“   \n       0      â†‘      â†      â†      â†      â†      â†      â†      â†   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n     Value Iteration\n    \n     Start from (0, 0) to Goal\n       ğŸ      â†’      â†“      0      0      0      0      0      0   \n       0      ğŸ      â†’      â†’      â†’      â†’      â†“      0      0   \n       0      â†‘      0      0      0      0      â†“      0      0   \n       0      â†‘      0      0      0      0      â†’      â†’      â†“   \n       0      â†‘      0      0      0      0      0      0      â†“   \n       0      â†‘      0      0      0      0      0      0      â†“   \n       0      â†‘      â†      â†      â†      â†      â†      â†      â†   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n     Q-Learning\n    \n     Start from (0, 0) to Goal\n       ğŸ      â†’      â†“      0      0      0      0      0      0   \n       0      0      â†’      â†’      â†’      â†’      â†’      â†’      â†“   \n       0      0      0      0      0      0      0      0      â†“   \n       0      0      0      â†“      â†      â†      â†      â†      â†   \n       0      0      â†“      â†      0      0      0      0      0   \n       0      â†“â†’     â†”      â†’      â†’      ğŸ      0      0      0   \n       0      â†‘      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n     SARSA\n    \n     Start from (0, 0) to Goal\n       ğŸ      0      0      0      0      0      0      0      0   \n       â†“      0      0      0      0      0      â†’      â†’      â†“   \n       â†“      0      0      0      0      0      â†‘      0      â†“   \n       â†’      â†“      0      0      â†’      â†’      â†‘      0      â†“   \n       0      â†“      0      â†’      â†‘      â†“      â†      â†      â†   \n       0      â†“      â†’      â†‘      0      ğŸ      0      0      0   \n       0      â†’      â†‘      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n     N-Step SARSA\n    \n     Start from (0, 0) to Goal\n       ğŸ      0      0      0      0      0      0      0      0   \n       â†“      0      0      0      0      0      â†“      â†      â†   \n       â†“      0      0      0      0      0      â†“      0      â†‘   \n       â†’      â†’      â†“      0      â†’      â†’      â†“â†’     â†’      â†‘   \n       0      0      â†“      â†’      â†‘      0      â†“      0      0   \n       0      â†“      â†”      â†‘      0      ğŸ      â†      0      0   \n       0      â†’      â†‘      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n\n# å…­,ç½‘ç»œæ¨¡å‹\n\n\n```python\nimport  torch\nimport  torch.nn as nn\nimport  torch.optim as optim\n```\n\n\n```python\na=torch.randint(0,10,(10,),dtype=torch.float32)\nnet=torch.nn.Linear(10,5)\nb=net(torch.tensor([1]*10,dtype=torch.float32))\nprint(b)\n```\n\n    tensor([ 0.5811, -0.2161,  0.4089,  0.6695, -0.6427], grad_fn=<ViewBackward0>)\n    \n\n\n```python\nclass DQN(nn.Module):\n    def __init__(self, input_dim, num_actions):\n        super(DQN, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, num_actions)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n```\n\n\n```python\nimport random\nfrom collections import  deque\nclass DQNAgent:\n    def __init__(self, input_dim, num_actions):\n        self.num_actions = num_actions\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n        self.policy_net = DQN(input_dim, num_actions).to(self.device)\n        self.target_net = DQN(input_dim, num_actions).to(self.device)\n        self.target_net.load_state_dict(self.policy_net.state_dict())\n        self.target_net.eval()\n\n        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=1e-3,  weight_decay=1e-5)\n        # self.scheduler=  optim.lr_scheduler.ReduceLROnPlateau(\n        #                     optimizer=self.optimizer,\n        #                     mode=\'min\',        # ç›‘æ§çš„æŒ‡æ ‡æ˜¯ lossï¼Œè¶Šå°è¶Šå¥½\n        #                     factor=0.5,        # å­¦ä¹ ç‡ä¹˜ä»¥ 0.5\n        #                     patience=10,       # ç­‰å¾… 10 ä¸ª episode loss æ²¡æœ‰ä¸‹é™æ‰è§¦å‘è¡°å‡\n        #                     min_lr=1e-5        # æœ€å°å­¦ç‡é™åˆ¶\n        #                 )\n        self.memory = deque(maxlen=10000)\n        self.batch_size = 64\n        self.gamma = 0.99\n        self.epsilon = 1.0\n        self.epsilon_min = 0.1\n        self.epsilon_decay = 0.995\n\n    def select_action(self, state):\n\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(self.num_actions)\n        else:\n            with torch.no_grad():\n                state = torch.FloatTensor(state).to(self.device)\n                q_values = self.policy_net(state)\n                return q_values.argmax().item()\n\n    def store_transition(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def update(self):\n        if len(self.memory) < self.batch_size:\n            return\n\n        batch = random.sample(self.memory, self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n\n        states = torch.FloatTensor(np.array(states)).to(self.device)\n        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n        dones = torch.FloatTensor(dones).to(self.device)\n        current_q = self.policy_net(states).gather(1, actions).squeeze()\n        next_q = self.target_net(next_states).max(1)[0]\n        expected_q = rewards + (1 - dones) * self.gamma * next_q\n\n        loss = nn.MSELoss()(current_q, expected_q.detach())\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        # ğŸ‘‡ å°† loss ä¼ ç»™ scheduler\n        # self.scheduler.step(loss.item())\n\n        # # Epsilon decay\n        # self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n\n        # æ›´æ–°ç›®æ ‡ç½‘ç»œ\n        self.soft_update_target()\n        return loss\n\n    def soft_update_target(self, tau=0.01):\n        for target_param, policy_param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n            target_param.data.copy_(tau * policy_param.data + (1 - tau) * target_param.data)\n```\n\n\n```python\n\ndef enviroment_reset():\n    state=np.random.randint(SIZE*SIZE)\n    mask=0 if state not in CHEST_STATES  else CHEST_STATES[state]\n    return state,mask\ndef toTensor(state,mask):\n    state_onehot = np.zeros(SIZE * SIZE)\n    state_onehot[state] = 1\n    bin_mask=list(map(int,bin(mask)[2:].zfill(len(CHEST_STATES))))\n    mask_onehot = np.array(bin_mask)\n    return torch.tensor(np.concatenate([state_onehot, mask_onehot]), dtype=torch.float32)\ndef train_dqn(episodes=1000, input_dim=84, num_actions=4):\n    agent = DQNAgent(input_dim, num_actions)\n    rewards_history = []\n\n    for ep in range(episodes):\n        state,mask=enviroment_reset()\n        total_reward = 0\n        total_loss=0\n        tensor=toTensor(state,mask)\n        done = state in GOAL_STATE\n        if done:\n            continue\n        path_length=0\n        while not done and path_length<81:\n            path_length+=1\n            action = agent.select_action(tensor)\n            next_state,next_mask, reward, done= step(state,mask,action)\n            next_tensor=toTensor(next_state,next_mask)\n            agent.store_transition(tensor, action, reward, next_tensor, done)\n            loss=agent.update()\n            total_loss+=loss.item()  if loss is not None else 0\n            total_reward += reward\n            state = next_state\n            mask=next_mask\n            tensor=next_tensor\n        avg_loss=total_loss/path_length\n        rewards_history.append(avg_loss)\n        agent.epsilon=max(agent.epsilon*agent.epsilon_decay,agent.epsilon_min)\n        print(f"\\rEpisode {ep+1}, Total Reward: {total_reward/path_length:.2f} Total Loss: {avg_loss:.2f}, Epsilon: {agent.epsilon:.2f}", end="")\n        if 0<avg_loss<=0.01:\n            break\n    return agent, rewards_history\n```\n\n\n```python\nagent,  rewards_history = train_dqn()\n```\n\n    Episode 1000, Total Reward: 37.47 Total Loss: 7.97, Epsilon: 0.100118\n\n\n```python\nimport matplotlib.pyplot as plt\nplt.plot(moving_average(rewards_history))\nplt.xlabel("Episode")\nplt.ylabel("Total Loss")\nplt.title("Training Progress")\nplt.show()\n```\n\n\n    \n![png](output_39_0.png)\n    \n\n\n\n```python\npolicy=np.zeros(shape=(SIZE*SIZE,FULL_CHEST_MASK+1))\nagent.target_net.eval()\nfor i in range(SIZE*SIZE):\n    for j in range(FULL_CHEST_MASK+1):\n        policy[i][j]=np.argmax(agent.target_net(toTensor(i,j).to(\'cuda\')).cpu().detach().numpy())\n```\n\n\n```python\nprint_path(0,0,np.array(policy,dtype=int))\n```\n\n    \n     Start from (0, 0) to Goal\n       ğŸ      â†’      â†“      0      0      0      0      0      0   \n       0      0      â†’      â†’      â†’      â†’      â†“      0      0   \n       0      0      0      0      0      0      â†“      0      0   \n       0      0      0      0      0      0      â†’      â†’      â†“   \n       0      0      0      0      0      0      0      0      â†“   \n       0      0      0      0      0      ğŸ      â†“      â†      â†   \n       0      â†’      â†”      â†”      â†”      â†‘â†     â†      0      0   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n\n\n```python\nprint_policy(np.array(policy,dtype=int))\n```\n\n      â†’    â†’    â†“    â†’    â†“    X    â†“    â†    â†  \n      â†“    G    â†’    â†’    â†’    â†’    1    â†    â†  \n      â†’    â†“    â†‘    â†’    â†’    â†’    â†‘    X    â†“  \n      â†“    â†“    â†“    â†’    â†‘    â†‘    â†‘    â†’    2  \n      â†’    â†“    â†    â†“    â†‘    â†’    â†’    â†’    â†‘  \n      X    â†“    â†    â†    â†“    G    â†’    â†’    â†‘  \n      â†’    4    â†    â†    â†    â†    â†’    â†’    â†‘  \n      â†’    â†‘    â†‘    â†    â†    â†    â†‘    â†‘    â†‘  \n      â†‘    â†‘    â†    X    â†‘    â†    â†’    â†‘    G  \n    \n\n\n```python\n# torch.save(agent.target_net.state_dict(), "model.pt")\n```\n\n\n```python\n\n```\n'}}]);
//# sourceMappingURL=8521.dbc7e91d.js.map