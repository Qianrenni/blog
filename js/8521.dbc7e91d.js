"use strict";(self["webpackChunkmyblog"]=self["webpackChunkmyblog"]||[]).push([[8521],{98521:function(n,e,t){t.r(e),e["default"]='# 目录\n\n- [一,全局环境参数定义](#一全局环境参数定义)\n- [二,工具函数和奖励函数定义](#二工具函数和奖励函数定义)\n- [三,迭代方法](#三迭代方法)\n  - [1.蒙特卡洛方法](#1蒙特卡洛方法)\n    - [1.1 策略生成](#11-策略生成)\n    - [1.2 On-policy 方法](#12-on-policy-方法)\n    - [1.3 Off-policy 方法](#13-off-policy-方法)\n  - [2.动态规划方法](#2动态规划方法)\n    - [2.1  策略评估](#21--策略评估)\n    - [2.2 价值迭代](#22-价值迭代)\n  - [3.时分序列方法](#3时分序列方法)\n    - [3.1 Q-learning](#31-q-learning)\n    - [3.2 SARSA](#32-sarsa)\n    - [3.3 N-Sarsa](#33-n-sarsa)\n- [四,输出策略和价值](#四输出策略和价值)\n- [五,调试](#五调试)\n- [六,网络模型](#六网络模型)\n\n# 一,全局环境参数定义\n\n\n```python\nimport random\n\nimport numpy as np\nfrom collections import defaultdict\n\n# 参数设置（复用之前的定义）\nSIZE = 9\nACTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]\nACTIONS_NAMES = [\'↑\', \'↓\', \'←\', \'→\']\nDOUBLE_ACTIONS={frozenset(\'↑↓\'):\'↕\',frozenset(\'↑←\'):\'↑←\',frozenset(\'↑→\'):\'↑→\',\n                frozenset(\'←→\'):\'↔\',frozenset(\'←↓\'):\'←↓\',frozenset(\'↓→\'):\'↓→\'}\nNUM_ACTIONS = len(ACTIONS)\nGOAL_STATE= {80,10,50}\nDANGER_STATES = {5,25,45,75}  # 危险区域\nCHEST_STATES = {15:1,35:2,55:4}  # 键为状态编号，值为对应的二进制位\nFULL_CHEST_MASK = sum(CHEST_STATES.values())\nGAMMA = 0.9\nNUM_EPISODES = 5000  # 训练轮数\nEPSILON = 0.1  # 探索率\nEPSILON_START = 1.0\nEPSILON_END = 0.01\nEPSILON_DECAY = 0.995  # 每一轮乘这个因子\nTHRESHOLD=1e-4\n```\n\n# 二,工具函数和奖励函数定义\n\n\n```python\n\ndef to_state(row, col):\n    return row * SIZE + col\n\ndef from_state(s):\n    return np.divmod(s, SIZE)\ndef is_terminal(s, mask):\n    return s in GOAL_STATE and mask == FULL_CHEST_MASK\n\ndef step(s, mask, a):\n    # 与前面一致：返回 next_s, next_mask, reward, done\n    row, col = divmod(s, SIZE)\n    dr, dc = ACTIONS[a]\n    new_row = max(0, min(SIZE - 1, row + dr))\n    new_col = max(0, min(SIZE - 1, col + dc))\n    # if new_row==row and new_col==col:\n    #     return s, mask, -100, False\n    next_s = to_state(new_row, new_col)\n\n    next_mask = mask\n    reward =-1\n    done = False\n\n    if next_s in GOAL_STATE:\n        done = True\n        if mask == FULL_CHEST_MASK:\n            reward = 1000\n        else:\n            reward = -100\n            return s, mask, reward, done  # 回原地\n\n    elif next_s in DANGER_STATES:\n        reward = -100\n\n    elif next_s in CHEST_STATES:\n        bit = CHEST_STATES[next_s]\n        if not (next_mask & bit):\n            next_mask = mask | bit\n            reward = 50\n        else:\n            reward = -1\n\n    return next_s, next_mask, reward, done\n\n```\n\n# 三,迭代方法\n\n## 1.蒙特卡洛方法\n\n### 1.1 策略生成\n\n\n```python\n\ndef generate_episode(policy, epsilon=EPSILON):\n    # 生成一个 episode\n    episode = []\n    s = np.random.randint(SIZE * SIZE)\n    mask = 0\n    for _ in range(100):  # 最大长度限制\n        if is_terminal(s, mask):\n            break\n        # ε-greedy 策略选择动作\n        if np.random.rand() < epsilon:\n            a = np.random.choice(NUM_ACTIONS)\n        else:\n            a = policy[s, mask]\n        next_s, next_mask, r, done = step(s, mask, a)\n        episode.append((s, mask, a, r))\n        s, mask = next_s, next_mask\n    return episode\n\n# 动态 ε-greedy 策略生成 episode\n# ========================\ndef generate_episode_with_dynamic_epsilon(Q, epsilon):\n    episode = []\n    s = np.random.randint(SIZE * SIZE)\n    mask = 0\n    for _ in range(100):  # 防止死循环\n        if is_terminal(s, mask):\n            break\n        # ε-greedy 策略选择动作\n        if np.random.rand() < epsilon:\n            a = np.random.choice(NUM_ACTIONS)\n        else:\n            values = [Q.get((s, mask, ac), 0) for ac in range(NUM_ACTIONS)]\n            a = np.argmax(values) if max(values)!=0 else np.random.choice(NUM_ACTIONS)\n        next_s, next_mask, r, done = step(s, mask, a)\n        episode.append((s, mask, a, r))\n        s, mask = next_s, next_mask\n    return episode\n```\n\n### 1.2 On-policy 方法\n\n\n```python\n\ndef on_policy_monte_carlo_control():\n    num_states = SIZE * SIZE\n    num_masks = FULL_CHEST_MASK + 1\n\n    # 初始化 Q 表和策略\n    Q = defaultdict(float)\n    C = defaultdict(float)\n    policy = np.zeros(shape=(num_states, num_masks), dtype=int)\n    episode_returns = []\n    for ep in range(NUM_EPISODES):\n        episode = generate_episode(policy,max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** ep)))\n\n        G = 0\n        visited_sa = set()\n\n        # 从后往前计算 G\n        for t in reversed(range(len(episode))):\n            s, mask, a, r = episode[t]\n            G = GAMMA * G + r\n            sa = (s, mask, a)\n\n            if sa not in visited_sa:\n                visited_sa.add(sa)\n                C[sa] += 1\n                Q[sa] += (G - Q[sa]) / C[sa]  # 增量平均更新\n        episode_returns.append(G)\n        # 策略改进（greedy）\n        for s in range(num_states):\n            for mask in range(num_masks):\n                if is_terminal(s, mask):\n                    continue\n                values = [Q.get((s, mask, a), 0) for a in range(NUM_ACTIONS)]\n                best_a = np.argmax(values)\n                policy[s, mask] = best_a\n\n        if ep % 1000 == 0:\n            print(f"Episode {ep} completed")\n\n    return policy, Q , episode_returns\n```\n\n### 1.3 Off-policy 方法\n\n\n```python\n\n# ========================\n# Off-policy First-Visit MC Control\n# ========================\ndef off_policy_monte_carlo():\n    num_states = SIZE * SIZE\n    num_masks = FULL_CHEST_MASK + 1\n\n    # 初始化\n    Q = defaultdict(float)\n    C = defaultdict(float)\n    target_policy = np.random.choice(NUM_ACTIONS, size=(num_states, num_masks))\n    episode_returns = []\n    for ep in range(NUM_EPISODES):\n        # 动态调整 epsilon\n        epsilon = max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** ep))\n        # 使用固定策略 π_b（behavior policy）收集数据\n        episode = generate_episode_with_dynamic_epsilon(Q, epsilon)\n        G = 0.0\n        W = 1.0  # 重要性采样比率\n        visited_sa = set()\n\n        # 从后往前处理每个时间步\n        for t in reversed(range(len(episode))):\n            s, mask, a, r = episode[t]\n            G = GAMMA * G + r\n            sa = (s, mask, a)\n\n            if sa not in visited_sa:\n                visited_sa.add(sa)\n\n                # 更新重要性采样统计量\n                C[sa] += W\n\n                # 增量平均更新 Q 值\n                Q[sa] += (W / C[sa]) * (G - Q[sa])\n\n                # 更新目标策略为 greedy\n                values = [Q.get((s, mask, ac), 0) for ac in range(NUM_ACTIONS)]\n                best_a = np.argmax(values)\n                target_policy[s, mask] = best_a\n\n                # 如果当前动作不是目标策略选的动作，则终止更新路径\n                if a != target_policy[s, mask]:\n                    W = 1e-8  # 权重归零，不再更新前面的状态\n                    break\n            else:\n                continue\n        episode_returns.append(G)\n        if ep % 500 == 0:\n            print(f"Episode {ep} completed")\n\n    return target_policy, Q,  episode_returns\n```\n\n## 2.动态规划方法\n\n### 2.1  策略评估\n\n\n```python\n\ndef policy_evaluation(policy,V,env_step=step):\n    value_deltas = []\n    while True:\n        delta = 0\n        V_new=np.zeros_like(V)\n        for s in range(SIZE*SIZE):\n            for mask in range(FULL_CHEST_MASK + 1):\n                if is_terminal(s, mask):  # 如果是终点且宝箱集齐\n                    continue\n                v = V[s, mask]\n                a = policy[s, mask]\n\n                # 环境转移函数应改为接受 (s, mask, a) 返回 (ns, nmask, r, done)\n                ns, nmask, r, done = env_step(s, mask, a)\n                V_new[s,mask]=r + GAMMA * V[ns, nmask]\n                delta = max(delta, abs(v - V_new[s, mask]))\n        value_deltas.append(delta)\n        V = V_new\n        if delta < THRESHOLD:\n            break\n\n    return V, value_deltas\ndef policy_improvement(policy,V,env_step=step):\n    stable = True\n    for s in range(SIZE*SIZE):\n        for mask in range(FULL_CHEST_MASK+1):\n            if is_terminal(s, mask):\n                continue\n            old_action = policy[s, mask]\n            values = []\n            for a in range(NUM_ACTIONS):\n                ns, nmask, r, done = env_step(s, mask, a)\n                values.append(r + GAMMA * V[ns, nmask])\n            best_a = np.argmax(values)\n            # 关键点 两个值相差不大会影响 策略评估,从而导致震荡\n            if best_a != old_action and values[best_a] - values[old_action] >0.2:\n                policy[s, mask] = best_a\n                stable = False\n    return policy, stable\ndef policy_iteration():\n    # 初始化策略和价值函数（现在是二维）\n    # 扩展状态总数 = 原始状态数 × 宝箱掩码数量\n    num_states = SIZE * SIZE\n    num_masks = FULL_CHEST_MASK + 1  # 从 0 到 FULL_CHEST_MASK\n    policy = np.zeros(shape=(num_states, num_masks), dtype=int)\n    V = np.zeros(shape=(num_states, num_masks))\n    deltas_history = []\n    for i in range(100):\n        V,deltas=policy_evaluation(policy,V)\n        deltas_history.extend(deltas)\n        policy,stable=policy_improvement(policy,V)\n        if stable:\n            print(f\'Policy iteration converged at iteration {i}\')\n            break\n    return policy,V,deltas_history\n```\n\n### 2.2 价值迭代\n\n\n```python\n\ndef value_iteration():\n    num_state=SIZE*SIZE\n    V=np.zeros(shape=(num_state,FULL_CHEST_MASK+1))\n    policy=np.zeros(shape=(num_state,FULL_CHEST_MASK+1),dtype=int)\n    deltas_history=[]\n    while True:\n        delta=0\n        V_new=np.zeros_like(V)\n        for s in range(num_state):\n            for mask  in range(FULL_CHEST_MASK+1):\n                if is_terminal(s,mask):\n                    continue\n                values=[]\n                for a in range(NUM_ACTIONS):\n                    next_state,next_mask,reward,done=step(s,mask,a)\n                    if not done:\n                        values.append(reward+GAMMA*V[next_state,next_mask])\n                    else:\n                        values.append(reward)\n                new_value=max(values)\n                delta=max(delta,abs(new_value-V[s,mask]))\n                V_new[s,mask]=new_value\n        deltas_history.append(delta)\n        V=V_new\n        if delta<THRESHOLD:\n            print(f\'Value iteration converged at iteration {len(deltas_history)}\')\n            break\n\n    for s in range(num_state):\n        for mask in range(FULL_CHEST_MASK+1):\n            if is_terminal(s,mask):\n                policy[s,mask]=-1\n                continue\n            values=[]\n            for a in range(NUM_ACTIONS):\n                next_state,next_mask,reward,done=step(s,mask,a)\n                values.append(reward+GAMMA*V[next_state,next_mask])\n            policy[s,mask]=np.argmax(values)\n    return policy,V,deltas_history\n```\n\n## 3.时分序列方法\n\n###  3.1 Q-learning\n\n\n```python\ndef q_learing():\n    import numpy as np\n    # 假设你有 SIZE*SIZE 个状态，FULL_CHEST_MASK+1 种 mask，NUM_ACTIONS 个动作\n    Q = np.zeros(shape=(SIZE*SIZE, FULL_CHEST_MASK+1, NUM_ACTIONS))\n    GAMMA = 0.95     # 折扣因子\n    ALPHA = 0.1      # 学习率\n    EPSILON_START = 0.9\n    EPSILON_END = 0.1\n    EPSILON_DECAY = 0.995\n    EPISODES = 5000 # 总共训练多少 episode\n    MAX_STEPS = 100  # 每个 episode 最大步数\n    def choose_action(state, mask, Q, epsilon):\n        if np.random.rand() < epsilon:\n            return np.random.randint(NUM_ACTIONS)\n        else:\n            return np.argmax(Q[state, mask])\n    def enviroment_reset():\n        return np.random.randint(SIZE*SIZE)\n    def initial_mask(initial_state):\n        return 0 if initial_state not in CHEST_STATES else CHEST_STATES[initial_state]\n    gradient_history = []\n    for episode in range(EPISODES):\n        state = enviroment_reset()\n        mask = initial_mask(state)\n        delta=0\n        for _ in range(MAX_STEPS):\n            if state in GOAL_STATE:\n                break\n            epsilon = max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** episode))\n            action = choose_action(state, mask, Q, epsilon)\n\n            # 与环境交互得到下一个状态等信息\n            next_state, next_mask, reward, done = step(state,mask,action)\n\n            # Q-learning 更新公式\n            td_target = (reward + GAMMA * np.max(Q[next_state, next_mask]) )if not done else reward\n            td_error = td_target - Q[state, mask, action]\n            Q[state, mask, action] += ALPHA * td_error\n            delta=max(delta,abs(td_error))\n            state = next_state\n            mask = next_mask\n            if done:\n                break\n        gradient_history.append(delta)\n    return np.argmax(Q,  axis=2),  Q, gradient_history\n```\n\n### 3.2 SARSA\n\n\n```python\n\ndef sarsa():\n    import numpy as np\n    from collections import defaultdict\n\n    # 超参数\n    NUM_EPISODES = 5000\n    GAMMA = 0.99\n    ALPHA = 0.1      # 学习率\n    EPSILON_START = 0.5\n    EPSILON_END = 0.01\n    EPSILON_DECAY = 0.995\n    num_states = SIZE * SIZE\n    num_masks = FULL_CHEST_MASK + 1\n    def enviroment_reset():\n        state=np.random.randint(SIZE*SIZE)\n        return state,0 if state not in CHEST_STATES else CHEST_STATES[state]\n    # 初始化 Q 表格\n    Q = np.zeros(shape=(num_states, num_masks, NUM_ACTIONS))\n    episode_returns = []\n\n    for ep in range(NUM_EPISODES):\n        epsilon = max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** ep))\n\n        # 重置环境（你需要自己实现 reset 函数）\n        state, mask = enviroment_reset()  # 返回初始 state 和 mask\n\n        # ε-greedy 选择第一个动作\n        if np.random.rand() < epsilon:\n            action = np.random.choice(NUM_ACTIONS)\n        else:\n            action = np.argmax(Q[(state, mask)])\n\n        done = state in GOAL_STATE\n        total_return = 0\n\n        while not done:\n            # 执行当前动作，得到下一个状态、奖励等\n            next_state, next_mask, reward, done = step(state, mask, action)\n\n            # ε-greedy 选择下一个动作\n            if np.random.rand() < epsilon:\n                next_action = np.random.choice(NUM_ACTIONS)\n            else:\n                next_action = np.argmax(Q[(next_state, next_mask)])\n\n            # SARSA 更新公式\n            td_target = reward + GAMMA * Q[(next_state, next_mask)][next_action]\n            td_error = td_target - Q[(state, mask)][action]\n            Q[(state, mask)][action] += ALPHA * td_error\n\n            # 累计回报\n            total_return += td_error\n\n            # 更新状态和动作\n            state, mask = next_state, next_mask\n            action = next_action\n\n        episode_returns.append(total_return)\n\n        if ep % 500 == 0:\n            print(f"Episode {ep} completed")\n\n    return np.argmax(Q,  axis=2),Q, episode_returns\n```\n\n### 3.3 N-Sarsa\n\n\n```python\ndef n_step_sarsa(n=3, NUM_EPISODES=5000):\n    import numpy as np\n    from collections import defaultdict\n\n    # 超参数\n    GAMMA = 0.99\n    ALPHA = 0.1      # 学习率\n    EPSILON_START = 0.5\n    EPSILON_END = 0.01\n    EPSILON_DECAY = 0.995\n    num_states = SIZE * SIZE\n    num_masks = FULL_CHEST_MASK + 1\n    MAX_EPISODE_LENGTH = num_states+1  # 设置最大步数\n    def enviroment_reset():\n        state=np.random.randint(SIZE*SIZE)\n        return state,0 if state not in CHEST_STATES else CHEST_STATES[state]\n    # 初始化 Q 表格：Q[state][mask][action]\n    Q = np.zeros(shape=(num_states, num_masks, NUM_ACTIONS))\n\n    episode_returns = []\n\n    for ep in range(NUM_EPISODES):\n        epsilon = max(EPSILON_END, EPSILON_START * (EPSILON_DECAY ** ep))\n\n        # 初始化环境\n        state, mask = enviroment_reset()  # 返回初始 state 和 mask\n        done = state in GOAL_STATE\n\n        # ε-greedy 选择第一个动作\n        if np.random.rand() < epsilon:\n            action = np.random.choice(NUM_ACTIONS)\n        else:\n            action = np.argmax(Q[state, mask])\n\n        # 存储轨迹信息\n        states = [state]\n        masks = [mask]\n        actions = [action]\n        rewards = []\n\n        T = float(\'inf\')  # episode 的总步数（还未知）\n        t = 0  # 当前时间步\n        # 累计本 episode 的回报\n        total_return = 0\n        while True:\n            if t < T:\n                # 执行动作，得到下一个状态、奖励等\n                next_state, next_mask, reward, done = step(state, mask, action)\n                next_action=None\n                # 存储奖励\n                rewards.append(reward)\n\n                # 如果未结束，继续选动作\n                if not done or t < MAX_EPISODE_LENGTH:\n                    if np.random.rand() < epsilon:\n                        next_action = np.random.choice(NUM_ACTIONS)\n                    else:\n                        next_action = np.argmax(Q[next_state, next_mask])\n                    # 存储下一个状态和动作\n                    states.append(next_state)\n                    masks.append(next_mask)\n                    actions.append(next_action)\n                else:\n                    T = t + 1  # 设置 episode 结束时间\n\n                # 更新状态和动作\n                state, mask, action = next_state, next_mask, next_action\n\n            # 更新目标时间点 τ\n            tau = t - n + 1\n            if tau >= 0:\n                # 计算 G_t^(n)：n-step return\n                end = min(tau + n, T)\n                G = sum([GAMMA ** (k - tau) * rewards[k] for k in range(tau, end)])\n\n                if end < T:\n                    G += GAMMA ** n * Q[states[end], masks[end]][actions[end]]\n                delta=ALPHA * (\n                    G - Q[states[tau], masks[tau], actions[tau]]\n                )\n                # 更新 Q 值\n                Q[states[tau], masks[tau], actions[tau]] += delta\n                total_return += delta/len(states)\n\n            if tau == T - 1:\n                break\n\n            t += 1\n\n        episode_returns.append(total_return)\n\n        if ep % 500 == 0:\n            print(f"Episode {ep} completed")\n\n    # 返回最优策略（取 argmax）和 Q 表\n    return np.argmax(Q, axis=2), Q, episode_returns\n```\n\n# 四,输出策略和价值\n\n\n```python\n\ndef print_policy(policy,mask=0):\n    for i in range(SIZE):\n        row = ""\n        for j in range(SIZE):\n            s = to_state(i, j)\n            if s in GOAL_STATE:\n                cell = "G"\n            elif s in DANGER_STATES:\n                cell = "X"\n            elif s in CHEST_STATES:\n                cell = CHEST_STATES[s]\n            else:\n                cell = ACTIONS_NAMES[policy[s, mask]]\n            row += f"{cell:^5}"\n        print(row)\ndef print_values(V,mask=0):\n    for i in range(SIZE):\n        row = ""\n        for j in range(SIZE):\n            s = to_state(i, j)\n            # if s in GOAL_STATE:\n            #     cell = \'G\'\n            # elif s in DANGER_STATES:\n            #     cell = \'X\'\n            # elif s in CHEST_STATES:\n            #     cell = f"$"\n            # else:\n            cell = f"{V[s,mask]:.2f}"\n            row += f"{cell:^7}"  # 居中对齐，每个数字占7个字符宽度\n        print(row)\ndef print_environment():\n    for i in range(SIZE):\n        row = ""\n        for j in range(SIZE):\n            s = to_state(i, j)\n            cell = "0"\n            if s in GOAL_STATE:\n                cell = \'G\'\n            elif s in DANGER_STATES:\n                cell = \'X\'\n            elif s in CHEST_STATES:\n                cell = f"$"\n            else:\n                pass\n            row += f"{cell:^7}"  # 居中对齐，每个数字占7个字符宽度\n        print(row)\ndef print_path(x,y,policy):\n    path=[ ["0"  for _ in range(SIZE)] for _  in range(SIZE)]\n    state=to_state(x,y)\n    mask=0\n    if state in CHEST_STATES:\n        mask=CHEST_STATES[state]\n    times=0\n    while state not in GOAL_STATE:\n        times+=1\n        action=policy[state,mask]\n        next_state,mask,_,_=step(state,mask,action)\n        i,j=from_state(state)\n        actions=frozenset(ACTIONS_NAMES[action]+path[i][j])\n        path[i][j]=DOUBLE_ACTIONS.get(actions,ACTIONS_NAMES[action])\n        if state>=SIZE*SIZE or state<0 or state==next_state or times>SIZE*SIZE:\n            break\n        state=next_state\n    i,j=from_state(state)\n    path[i][j]="🏁"\n    path[x][y]="🏁"\n    for i in range(SIZE):\n        for j in range(SIZE):\n            path[i][j]=path[i][j].center(7)\n    print(f"\\n Start from {x,y} to Goal")\n    for row in path:\n        print("".join(row))\ndef moving_average(a, window_size=50):\n    return np.convolve(a, np.ones(window_size)/window_size, mode=\'valid\')\ndef show_gradient_history(gradient_history):\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(12,8))\n    plt.plot(moving_average(gradient_history))\n    plt.xlabel(\'Episode\')\n    plt.ylabel(\'TD Error\')\n    plt.title(\'TD Error History\')\n    plt.show()\n\n```\n\n# 五,调试\n\n\n```python\nprint("Environment:")\nprint_environment()\n\non_policy,on_Q,on_episode_returns=on_policy_monte_carlo_control()\nprint("\\n On Policy:")\nprint_policy(on_policy)\n\noff_policy, off_Q,off_episode_returns = off_policy_monte_carlo()\nprint("\\n Off Policy:")\nprint_policy(off_policy)\n\npolicy,V,deltas_history=policy_iteration()\nprint("\\n Policy Iteration:")\nprint_policy(policy)\n\nvalue_policy,value_V,value_deltas_history=value_iteration()\nprint("\\n Value Iteration:")\nprint_policy(value_policy)\nq_policy,Q,gradient_history=q_learing()\nprint("\\n Q-Learning:")\nprint_policy(q_policy)\nprint("\\n SARSA")\nsarsa_policy,sarsa_Q,sarsa_gradient_history=sarsa()\nprint_policy(sarsa_policy)\nprint("\\n N-Step SARSA")\nn_sarsa_policy,n_sarsa_Q,n_sarsa_gradient_history=n_step_sarsa()\nprint_policy(n_sarsa_policy)\n```\n\n    Environment:\n       0      0      0      0      0      X      0      0      0   \n       0      G      0      0      0      0      $      0      0   \n       0      0      0      0      0      0      0      X      0   \n       0      0      0      0      0      0      0      0      $   \n       0      0      0      0      0      0      0      0      0   \n       X      0      0      0      0      G      0      0      0   \n       0      $      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      X      0      0      0      0      G   \n    Episode 0 completed\n    Episode 1000 completed\n    Episode 2000 completed\n    Episode 3000 completed\n    Episode 4000 completed\n    \n     On Policy:\n      →    →    →    ↓    ←    X    →    →    ↓  \n      ↑    G    ↓    →    →    →    1    →    ↓  \n      →    ↓    ↓    ↓    ↑    ↑    ↑    X    ↓  \n      →    →    →    →    ↑    ↑    →    →    2  \n      ↑    →    →    ↓    ←    ↑    ↑    ←    ↑  \n      X    →    ↓    ←    ←    G    →    ↑    ←  \n      →    4    ←    →    ←    ←    →    ↑    ↑  \n      →    ↑    ←    ←    ←    ←    ↑    ↑    ↑  \n      ↑    →    ↑    X    ↑    ↓    ↑    ←    G  \n    Episode 0 completed\n    Episode 500 completed\n    Episode 1000 completed\n    Episode 1500 completed\n    Episode 2000 completed\n    Episode 2500 completed\n    Episode 3000 completed\n    Episode 3500 completed\n    Episode 4000 completed\n    Episode 4500 completed\n    \n     Off Policy:\n      →    →    →    →    →    X    ↓    ←    ←  \n      ↑    G    ↓    ←    →    →    1    ↑    ↓  \n      ↑    →    ↓    ↓    ↑    ←    ↑    X    ↓  \n      ↓    ↓    ←    →    ↑    →    ↓    →    2  \n      →    →    →    ↑    ↑    →    →    →    ↑  \n      X    ↑    ↑    ↑    ←    G    ↑    ↑    ↑  \n      →    4    →    ↓    →    →    ↑    →    ↑  \n      →    →    →    →    →    ↑    ↑    →    ↑  \n      →    ↑    ↑    X    →    ↑    ←    ↑    G  \n    Policy iteration converged at iteration 13\n    \n     Policy Iteration:\n      →    →    →    →    ↓    X    ↓    ↓    ←  \n      ↓    G    →    →    →    →    1    ←    ←  \n      ↓    ↓    ↑    ↑    ↑    ↑    ↑    X    ↓  \n      ↓    ↓    ←    ↑    ↑    ↑    ↑    →    2  \n      →    ↓    ↓    ↓    ↑    ↑    ↑    →    ↑  \n      X    ↓    ↓    ↓    ↓    G    ↑    ↑    ↑  \n      →    4    ←    ←    ←    ←    ↑    ↑    ↑  \n      ↑    ↑    ↑    ←    ←    ←    ↑    ↑    ↑  \n      →    ↑    ↑    X    ↑    ↑    →    ↑    G  \n    Value iteration converged at iteration 27\n    \n     Value Iteration:\n      →    →    ↓    ↓    ↓    X    ↓    ↓    ↓  \n      ↓    G    →    →    →    →    1    ←    ↓  \n      ↓    ↓    ↑    ↑    ↑    ↑    ↑    X    ↓  \n      ↓    ↓    ↓    ↑    ↑    ↑    ↑    →    2  \n      →    ↓    ↓    ↓    ↑    ↑    ↑    ↑    ↑  \n      X    ↓    ↓    ↓    ↓    G    ↑    ↑    ↑  \n      →    4    ←    ←    ←    ←    ↑    ↑    ↑  \n      ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑    ↑  \n      ↑    ↑    ↑    X    ↑    ↑    ↑    ↑    G  \n    \n     Q-Learning:\n      →    →    ↓    →    ↓    X    ↓    ↓    ←  \n      ↑    G    →    →    →    →    1    ←    ←  \n      ↓    ↓    ↑    →    ↑    ↑    ↑    X    ↓  \n      ↓    →    ↓    ←    ↑    ↑    ↑    →    2  \n      →    ↓    ↓    ↓    ←    ↑    →    →    ↑  \n      X    ↓    ↓    ←    ←    G    ↑    ↑    ↑  \n      →    4    ←    ←    ←    ←    ←    ↑    ↑  \n      ↑    ↑    ↑    ←    ↑    ←    ←    ↑    ↑  \n      →    ↑    ↑    X    ↑    ↑    ←    ↑    G  \n    \n     SARSA\n    Episode 0 completed\n    Episode 500 completed\n    Episode 1000 completed\n    Episode 1500 completed\n    Episode 2000 completed\n    Episode 2500 completed\n    Episode 3000 completed\n    Episode 3500 completed\n    Episode 4000 completed\n    Episode 4500 completed\n      ↓    →    ↓    →    ↓    X    ↓    ↓    ←  \n      ↓    G    →    →    →    →    1    ←    ←  \n      ↓    ↓    ↑    ↑    ↑    →    ↑    X    ↓  \n      →    ↓    ←    ←    →    →    ↑    →    2  \n      →    ↓    ↓    ↓    ←    ↑    ↑    →    ↑  \n      X    ↓    ←    ↓    ←    G    →    →    ↑  \n      →    4    ←    ←    ←    ←    →    →    ↑  \n      →    ↑    ↑    ←    ↑    ←    ←    →    ↑  \n      →    ↑    ↑    X    ↑    ←    ↑    ↑    G  \n    \n     N-Step SARSA\n    Episode 0 completed\n    Episode 500 completed\n    Episode 1000 completed\n    Episode 1500 completed\n    Episode 2000 completed\n    Episode 2500 completed\n    Episode 3000 completed\n    Episode 3500 completed\n    Episode 4000 completed\n    Episode 4500 completed\n      ↓    ←    ←    ←    ↓    X    ↓    ←    ←  \n      ↓    G    ↓    ↓    →    ↓    1    ↑    ←  \n      ↓    →    →    ↓    ↓    ↓    ↑    X    ↓  \n      →    →    ↓    →    →    ↓    ←    →    2  \n      ↑    ←    ↓    ↓    →    →    ↓    →    ↑  \n      X    ↓    ←    ←    ←    G    →    →    ↑  \n      →    4    ↑    ←    ←    ←    →    →    ↑  \n      ↑    ↑    ←    →    ↑    ←    ↑    →    ↑  \n      →    ↑    ←    X    ↑    →    ↑    ←    G  \n    \n\n\n```python\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,8))\nplt.plot(moving_average(on_episode_returns), label=\'On-policy\')\nplt.plot(moving_average(off_episode_returns), label=\'Off-policy\')\nplt.xlabel(\'Episode\')\nplt.ylabel(\'Average Return\')\nplt.legend()\n\nplt.show()\n```\n\n\n    \n![png](output_28_0.png)\n    \n\n\n\n```python\nplt.figure(figsize=(12,8))\nplt.plot(value_deltas_history, label=\'value-policy\')\nplt.plot(deltas_history, label=\'policy\')\nplt.xlabel(\'Episode\')\nplt.ylabel(\'Delta Decay\')\nplt.legend()\nplt.show()\n```\n\n\n    \n![png](output_29_0.png)\n    \n\n\n\n```python\nplt.figure(figsize=(12,8))\nplt.plot(moving_average(gradient_history), label=\'Q-Learning\')\nplt.plot(moving_average(sarsa_gradient_history), label=\'SARSA\')\nplt.plot(moving_average(n_sarsa_gradient_history), label=\'N-Step SARSA\')\nplt.xlabel(\'Episode\')\nplt.ylabel(\'TD Error\')\nplt.legend()\nplt.show()\n```\n\n\n    \n![png](output_30_0.png)\n    \n\n\n\n```python\nstart_pos=(0,0)\nprint("\\n On Policy")\nprint_path(*start_pos,on_policy)\nprint("\\n Off Policy")\nprint_path(*start_pos,off_policy)\nprint("\\n Policy Iteration")\nprint_path(*start_pos,policy)\nprint("\\n Value Iteration")\nprint_path(*start_pos,value_policy)\nprint("\\n Q-Learning")\nprint_path(*start_pos,q_policy)\nprint("\\n SARSA")\nprint_path(*start_pos,sarsa_policy)\nprint("\\n N-Step SARSA")\nprint_path(*start_pos,n_sarsa_policy)\n```\n\n    \n     On Policy\n    \n     Start from (0, 0) to Goal\n       🏁      →      →      ↓      0      0      0      0      0   \n       0      0      0      →      →      →      →      →      ↓   \n       0      0      0      0      0      0      0      0      ↓   \n       0      0      0      0      0      0      0      0      ↓   \n       0      0      0      0      0      0      0      ↓      ←   \n       0      0      0      0      0      🏁      0      ↓      0   \n       0      →      →      →      →      ↑      ↓      ←      0   \n       0      ↑      ←      ←      ←      ←      ←      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n     Off Policy\n    \n     Start from (0, 0) to Goal\n       🏁      →      →      →      →      →      ↓      0      0   \n       0      0      0      0      0      0      →      →      ↓   \n       0      0      0      0      0      0      0      0      ↓   \n       0      0      0      0      0      0      0      0      ↓   \n       0      0      0      0      0      0      0      0      ↓   \n       0      0      →      →      →      🏁      0      ↓      ←   \n       0      →      ↑←     ←      ←      ←      ←      ←      0   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n     Policy Iteration\n    \n     Start from (0, 0) to Goal\n       🏁      →      →      →      ↓      0      0      0      0   \n       0      🏁      0      0      →      →      ↓      0      0   \n       0      ↑      0      0      0      0      ↓      0      0   \n       0      ↑      0      0      0      0      →      →      ↓   \n       0      ↑      0      0      0      0      0      0      ↓   \n       0      ↑      0      0      0      0      0      0      ↓   \n       0      ↑      ←      ←      ←      ←      ←      ←      ←   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n     Value Iteration\n    \n     Start from (0, 0) to Goal\n       🏁      →      ↓      0      0      0      0      0      0   \n       0      🏁      →      →      →      →      ↓      0      0   \n       0      ↑      0      0      0      0      ↓      0      0   \n       0      ↑      0      0      0      0      →      →      ↓   \n       0      ↑      0      0      0      0      0      0      ↓   \n       0      ↑      0      0      0      0      0      0      ↓   \n       0      ↑      ←      ←      ←      ←      ←      ←      ←   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n     Q-Learning\n    \n     Start from (0, 0) to Goal\n       🏁      →      ↓      0      0      0      0      0      0   \n       0      0      →      →      →      →      →      →      ↓   \n       0      0      0      0      0      0      0      0      ↓   \n       0      0      0      ↓      ←      ←      ←      ←      ←   \n       0      0      ↓      ←      0      0      0      0      0   \n       0      ↓→     ↔      →      →      🏁      0      0      0   \n       0      ↑      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n     SARSA\n    \n     Start from (0, 0) to Goal\n       🏁      0      0      0      0      0      0      0      0   \n       ↓      0      0      0      0      0      →      →      ↓   \n       ↓      0      0      0      0      0      ↑      0      ↓   \n       →      ↓      0      0      →      →      ↑      0      ↓   \n       0      ↓      0      →      ↑      ↓      ←      ←      ←   \n       0      ↓      →      ↑      0      🏁      0      0      0   \n       0      →      ↑      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n     N-Step SARSA\n    \n     Start from (0, 0) to Goal\n       🏁      0      0      0      0      0      0      0      0   \n       ↓      0      0      0      0      0      ↓      ←      ←   \n       ↓      0      0      0      0      0      ↓      0      ↑   \n       →      →      ↓      0      →      →      ↓→     →      ↑   \n       0      0      ↓      →      ↑      0      ↓      0      0   \n       0      ↓      ↔      ↑      0      🏁      ←      0      0   \n       0      →      ↑      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n\n# 六,网络模型\n\n\n```python\nimport  torch\nimport  torch.nn as nn\nimport  torch.optim as optim\n```\n\n\n```python\na=torch.randint(0,10,(10,),dtype=torch.float32)\nnet=torch.nn.Linear(10,5)\nb=net(torch.tensor([1]*10,dtype=torch.float32))\nprint(b)\n```\n\n    tensor([ 0.5811, -0.2161,  0.4089,  0.6695, -0.6427], grad_fn=<ViewBackward0>)\n    \n\n\n```python\nclass DQN(nn.Module):\n    def __init__(self, input_dim, num_actions):\n        super(DQN, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, num_actions)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n```\n\n\n```python\nimport random\nfrom collections import  deque\nclass DQNAgent:\n    def __init__(self, input_dim, num_actions):\n        self.num_actions = num_actions\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n        self.policy_net = DQN(input_dim, num_actions).to(self.device)\n        self.target_net = DQN(input_dim, num_actions).to(self.device)\n        self.target_net.load_state_dict(self.policy_net.state_dict())\n        self.target_net.eval()\n\n        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=1e-3,  weight_decay=1e-5)\n        # self.scheduler=  optim.lr_scheduler.ReduceLROnPlateau(\n        #                     optimizer=self.optimizer,\n        #                     mode=\'min\',        # 监控的指标是 loss，越小越好\n        #                     factor=0.5,        # 学习率乘以 0.5\n        #                     patience=10,       # 等待 10 个 episode loss 没有下降才触发衰减\n        #                     min_lr=1e-5        # 最小学率限制\n        #                 )\n        self.memory = deque(maxlen=10000)\n        self.batch_size = 64\n        self.gamma = 0.99\n        self.epsilon = 1.0\n        self.epsilon_min = 0.1\n        self.epsilon_decay = 0.995\n\n    def select_action(self, state):\n\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(self.num_actions)\n        else:\n            with torch.no_grad():\n                state = torch.FloatTensor(state).to(self.device)\n                q_values = self.policy_net(state)\n                return q_values.argmax().item()\n\n    def store_transition(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def update(self):\n        if len(self.memory) < self.batch_size:\n            return\n\n        batch = random.sample(self.memory, self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n\n        states = torch.FloatTensor(np.array(states)).to(self.device)\n        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n        dones = torch.FloatTensor(dones).to(self.device)\n        current_q = self.policy_net(states).gather(1, actions).squeeze()\n        next_q = self.target_net(next_states).max(1)[0]\n        expected_q = rewards + (1 - dones) * self.gamma * next_q\n\n        loss = nn.MSELoss()(current_q, expected_q.detach())\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        # 👇 将 loss 传给 scheduler\n        # self.scheduler.step(loss.item())\n\n        # # Epsilon decay\n        # self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n\n        # 更新目标网络\n        self.soft_update_target()\n        return loss\n\n    def soft_update_target(self, tau=0.01):\n        for target_param, policy_param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n            target_param.data.copy_(tau * policy_param.data + (1 - tau) * target_param.data)\n```\n\n\n```python\n\ndef enviroment_reset():\n    state=np.random.randint(SIZE*SIZE)\n    mask=0 if state not in CHEST_STATES  else CHEST_STATES[state]\n    return state,mask\ndef toTensor(state,mask):\n    state_onehot = np.zeros(SIZE * SIZE)\n    state_onehot[state] = 1\n    bin_mask=list(map(int,bin(mask)[2:].zfill(len(CHEST_STATES))))\n    mask_onehot = np.array(bin_mask)\n    return torch.tensor(np.concatenate([state_onehot, mask_onehot]), dtype=torch.float32)\ndef train_dqn(episodes=1000, input_dim=84, num_actions=4):\n    agent = DQNAgent(input_dim, num_actions)\n    rewards_history = []\n\n    for ep in range(episodes):\n        state,mask=enviroment_reset()\n        total_reward = 0\n        total_loss=0\n        tensor=toTensor(state,mask)\n        done = state in GOAL_STATE\n        if done:\n            continue\n        path_length=0\n        while not done and path_length<81:\n            path_length+=1\n            action = agent.select_action(tensor)\n            next_state,next_mask, reward, done= step(state,mask,action)\n            next_tensor=toTensor(next_state,next_mask)\n            agent.store_transition(tensor, action, reward, next_tensor, done)\n            loss=agent.update()\n            total_loss+=loss.item()  if loss is not None else 0\n            total_reward += reward\n            state = next_state\n            mask=next_mask\n            tensor=next_tensor\n        avg_loss=total_loss/path_length\n        rewards_history.append(avg_loss)\n        agent.epsilon=max(agent.epsilon*agent.epsilon_decay,agent.epsilon_min)\n        print(f"\\rEpisode {ep+1}, Total Reward: {total_reward/path_length:.2f} Total Loss: {avg_loss:.2f}, Epsilon: {agent.epsilon:.2f}", end="")\n        if 0<avg_loss<=0.01:\n            break\n    return agent, rewards_history\n```\n\n\n```python\nagent,  rewards_history = train_dqn()\n```\n\n    Episode 1000, Total Reward: 37.47 Total Loss: 7.97, Epsilon: 0.100118\n\n\n```python\nimport matplotlib.pyplot as plt\nplt.plot(moving_average(rewards_history))\nplt.xlabel("Episode")\nplt.ylabel("Total Loss")\nplt.title("Training Progress")\nplt.show()\n```\n\n\n    \n![png](output_39_0.png)\n    \n\n\n\n```python\npolicy=np.zeros(shape=(SIZE*SIZE,FULL_CHEST_MASK+1))\nagent.target_net.eval()\nfor i in range(SIZE*SIZE):\n    for j in range(FULL_CHEST_MASK+1):\n        policy[i][j]=np.argmax(agent.target_net(toTensor(i,j).to(\'cuda\')).cpu().detach().numpy())\n```\n\n\n```python\nprint_path(0,0,np.array(policy,dtype=int))\n```\n\n    \n     Start from (0, 0) to Goal\n       🏁      →      ↓      0      0      0      0      0      0   \n       0      0      →      →      →      →      ↓      0      0   \n       0      0      0      0      0      0      ↓      0      0   \n       0      0      0      0      0      0      →      →      ↓   \n       0      0      0      0      0      0      0      0      ↓   \n       0      0      0      0      0      🏁      ↓      ←      ←   \n       0      →      ↔      ↔      ↔      ↑←     ←      0      0   \n       0      0      0      0      0      0      0      0      0   \n       0      0      0      0      0      0      0      0      0   \n    \n\n\n```python\nprint_policy(np.array(policy,dtype=int))\n```\n\n      →    →    ↓    →    ↓    X    ↓    ←    ←  \n      ↓    G    →    →    →    →    1    ←    ←  \n      →    ↓    ↑    →    →    →    ↑    X    ↓  \n      ↓    ↓    ↓    →    ↑    ↑    ↑    →    2  \n      →    ↓    ←    ↓    ↑    →    →    →    ↑  \n      X    ↓    ←    ←    ↓    G    →    →    ↑  \n      →    4    ←    ←    ←    ←    →    →    ↑  \n      →    ↑    ↑    ←    ←    ←    ↑    ↑    ↑  \n      ↑    ↑    ←    X    ↑    ←    →    ↑    G  \n    \n\n\n```python\n# torch.save(agent.target_net.state_dict(), "model.pt")\n```\n\n\n```python\n\n```\n'}}]);
//# sourceMappingURL=8521.dbc7e91d.js.map